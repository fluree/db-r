//! Fast vector math helpers (scalar + SIMD runtime dispatch).
//!
//! Used by filter functions like `dotProduct`, `cosineSimilarity`, and `euclideanDistance`.
//!
//! Design goals:
//! - **No user configuration**: runtime dispatch uses SIMD when available.
//! - **Portable**: scalar fallback on all platforms.
//! - **Safe call sites**: SIMD functions are `unsafe` + guarded by feature detection.

#[inline]
pub fn dot_f64(a: &[f64], b: &[f64]) -> f64 {
    debug_assert_eq!(a.len(), b.len());

    #[cfg(target_arch = "x86_64")]
    {
        // AVX (not AVX2) is sufficient for f64 mul/add.
        if std::arch::is_x86_feature_detected!("avx") {
            // SAFETY: guarded by runtime feature detection.
            return unsafe { dot_f64_avx(a, b) };
        }

        // SSE2 is baseline on x86_64.
        return unsafe { dot_f64_sse2(a, b) };
    }

    #[cfg(target_arch = "aarch64")]
    {
        // NEON/ASIMD is baseline on aarch64.
        return unsafe { dot_f64_neon(a, b) };
    }

    #[cfg(not(any(target_arch = "x86_64", target_arch = "aarch64")))]
    dot_f64_scalar(a, b)
}

#[inline]
pub fn l2_f64(a: &[f64], b: &[f64]) -> f64 {
    debug_assert_eq!(a.len(), b.len());

    #[cfg(target_arch = "x86_64")]
    {
        if std::arch::is_x86_feature_detected!("avx") {
            return unsafe { l2_f64_avx(a, b) };
        }
        return unsafe { l2_f64_sse2(a, b) };
    }

    #[cfg(target_arch = "aarch64")]
    {
        return unsafe { l2_f64_neon(a, b) };
    }

    #[cfg(not(any(target_arch = "x86_64", target_arch = "aarch64")))]
    l2_f64_scalar(a, b)
}

#[inline]
pub fn cosine_f64(a: &[f64], b: &[f64]) -> Option<f64> {
    debug_assert_eq!(a.len(), b.len());

    // We compute dot + squared magnitudes in a single pass for cache efficiency.
    let (dot, mag_a2, mag_b2) = dot_mag2_f64(a, b);
    if mag_a2 == 0.0 || mag_b2 == 0.0 {
        None
    } else {
        Some(dot / (mag_a2.sqrt() * mag_b2.sqrt()))
    }
}

#[inline]
fn dot_mag2_f64(a: &[f64], b: &[f64]) -> (f64, f64, f64) {
    debug_assert_eq!(a.len(), b.len());

    // SIMD variants could be added, but start with a scalar single-pass loop.
    // This already saves an extra pass vs separate dot + norm loops.
    let mut dot = 0.0;
    let mut mag_a2 = 0.0;
    let mut mag_b2 = 0.0;
    for i in 0..a.len() {
        let x = a[i];
        let y = b[i];
        dot += x * y;
        mag_a2 += x * x;
        mag_b2 += y * y;
    }
    (dot, mag_a2, mag_b2)
}

#[cfg(any(test, not(any(target_arch = "x86_64", target_arch = "aarch64"))))]
#[inline]
fn dot_f64_scalar(a: &[f64], b: &[f64]) -> f64 {
    let mut acc = 0.0;
    for i in 0..a.len() {
        acc += a[i] * b[i];
    }
    acc
}

#[cfg(any(test, not(any(target_arch = "x86_64", target_arch = "aarch64"))))]
#[inline]
fn l2_f64_scalar(a: &[f64], b: &[f64]) -> f64 {
    let mut acc = 0.0;
    for i in 0..a.len() {
        let d = a[i] - b[i];
        acc += d * d;
    }
    acc.sqrt()
}

// =============================================================================
// x86_64 SIMD
// =============================================================================

#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "sse2")]
unsafe fn dot_f64_sse2(a: &[f64], b: &[f64]) -> f64 {
    use std::arch::x86_64::*;

    let mut acc = _mm_setzero_pd();
    let mut i = 0usize;
    let n = a.len();
    while i + 2 <= n {
        let va = _mm_loadu_pd(a.as_ptr().add(i));
        let vb = _mm_loadu_pd(b.as_ptr().add(i));
        acc = _mm_add_pd(acc, _mm_mul_pd(va, vb));
        i += 2;
    }

    // Horizontal sum
    let mut tmp = [0f64; 2];
    _mm_storeu_pd(tmp.as_mut_ptr(), acc);
    let mut sum = tmp[0] + tmp[1];

    // Tail
    while i < n {
        sum += *a.get_unchecked(i) * *b.get_unchecked(i);
        i += 1;
    }

    sum
}

#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "avx")]
unsafe fn dot_f64_avx(a: &[f64], b: &[f64]) -> f64 {
    use std::arch::x86_64::*;

    let mut acc = _mm256_setzero_pd();
    let mut i = 0usize;
    let n = a.len();
    while i + 4 <= n {
        let va = _mm256_loadu_pd(a.as_ptr().add(i));
        let vb = _mm256_loadu_pd(b.as_ptr().add(i));
        acc = _mm256_add_pd(acc, _mm256_mul_pd(va, vb));
        i += 4;
    }

    // Horizontal sum of 4 lanes.
    let hi = _mm256_extractf128_pd(acc, 1);
    let lo = _mm256_castpd256_pd128(acc);
    let sum2 = _mm_add_pd(lo, hi);
    let mut tmp = [0f64; 2];
    _mm_storeu_pd(tmp.as_mut_ptr(), sum2);
    let mut sum = tmp[0] + tmp[1];

    // Tail
    while i < n {
        sum += *a.get_unchecked(i) * *b.get_unchecked(i);
        i += 1;
    }

    sum
}

#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "sse2")]
unsafe fn l2_f64_sse2(a: &[f64], b: &[f64]) -> f64 {
    use std::arch::x86_64::*;

    let mut acc = _mm_setzero_pd();
    let mut i = 0usize;
    let n = a.len();
    while i + 2 <= n {
        let va = _mm_loadu_pd(a.as_ptr().add(i));
        let vb = _mm_loadu_pd(b.as_ptr().add(i));
        let d = _mm_sub_pd(va, vb);
        acc = _mm_add_pd(acc, _mm_mul_pd(d, d));
        i += 2;
    }

    let mut tmp = [0f64; 2];
    _mm_storeu_pd(tmp.as_mut_ptr(), acc);
    let mut sum = tmp[0] + tmp[1];

    while i < n {
        let d = *a.get_unchecked(i) - *b.get_unchecked(i);
        sum += d * d;
        i += 1;
    }

    sum.sqrt()
}

#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "avx")]
unsafe fn l2_f64_avx(a: &[f64], b: &[f64]) -> f64 {
    use std::arch::x86_64::*;

    let mut acc = _mm256_setzero_pd();
    let mut i = 0usize;
    let n = a.len();
    while i + 4 <= n {
        let va = _mm256_loadu_pd(a.as_ptr().add(i));
        let vb = _mm256_loadu_pd(b.as_ptr().add(i));
        let d = _mm256_sub_pd(va, vb);
        acc = _mm256_add_pd(acc, _mm256_mul_pd(d, d));
        i += 4;
    }

    let hi = _mm256_extractf128_pd(acc, 1);
    let lo = _mm256_castpd256_pd128(acc);
    let sum2 = _mm_add_pd(lo, hi);
    let mut tmp = [0f64; 2];
    _mm_storeu_pd(tmp.as_mut_ptr(), sum2);
    let mut sum = tmp[0] + tmp[1];

    while i < n {
        let d = *a.get_unchecked(i) - *b.get_unchecked(i);
        sum += d * d;
        i += 1;
    }

    sum.sqrt()
}

// =============================================================================
// aarch64 SIMD (NEON)
// =============================================================================

#[cfg(target_arch = "aarch64")]
#[target_feature(enable = "neon")]
unsafe fn dot_f64_neon(a: &[f64], b: &[f64]) -> f64 {
    use std::arch::aarch64::*;

    let mut acc = vdupq_n_f64(0.0);
    let mut i = 0usize;
    let n = a.len();
    while i + 2 <= n {
        let va = vld1q_f64(a.as_ptr().add(i));
        let vb = vld1q_f64(b.as_ptr().add(i));
        acc = vaddq_f64(acc, vmulq_f64(va, vb));
        i += 2;
    }

    let mut sum = vaddvq_f64(acc);
    while i < n {
        sum += *a.get_unchecked(i) * *b.get_unchecked(i);
        i += 1;
    }
    sum
}

#[cfg(target_arch = "aarch64")]
#[target_feature(enable = "neon")]
unsafe fn l2_f64_neon(a: &[f64], b: &[f64]) -> f64 {
    use std::arch::aarch64::*;

    let mut acc = vdupq_n_f64(0.0);
    let mut i = 0usize;
    let n = a.len();
    while i + 2 <= n {
        let va = vld1q_f64(a.as_ptr().add(i));
        let vb = vld1q_f64(b.as_ptr().add(i));
        let d = vsubq_f64(va, vb);
        acc = vaddq_f64(acc, vmulq_f64(d, d));
        i += 2;
    }

    let mut sum = vaddvq_f64(acc);
    while i < n {
        let d = *a.get_unchecked(i) - *b.get_unchecked(i);
        sum += d * d;
        i += 1;
    }
    sum.sqrt()
}

// =============================================================================
// Tests (scalar correctness)
// =============================================================================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn dot_matches_scalar() {
        let a = vec![0.6, 0.5, -1.0, 2.0, 3.25];
        let b = vec![0.7, -0.25, 4.0, 1.5, 0.5];
        let expected = dot_f64_scalar(&a, &b);
        let got = dot_f64(&a, &b);
        assert!((got - expected).abs() < 1e-12, "got {got}, expected {expected}");
    }

    #[test]
    fn l2_matches_scalar() {
        let a = vec![0.6, 0.5, -1.0, 2.0, 3.25];
        let b = vec![0.7, -0.25, 4.0, 1.5, 0.5];
        let expected = l2_f64_scalar(&a, &b);
        let got = l2_f64(&a, &b);
        assert!((got - expected).abs() < 1e-12, "got {got}, expected {expected}");
    }

    #[test]
    fn cosine_handles_zero_vectors() {
        let a = vec![0.0, 0.0];
        let b = vec![1.0, 2.0];
        assert_eq!(cosine_f64(&a, &b), None);
    }
}

