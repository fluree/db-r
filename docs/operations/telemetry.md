# Telemetry and Logging

Fluree provides comprehensive logging, metrics, and tracing capabilities for monitoring and debugging production deployments.

## Logging

### Log Levels

Configure log verbosity:

```bash
--log-level error|warn|info|debug|trace
```

**error:** Critical errors only
**warn:** Warnings and errors
**info:** Informational messages (default)
**debug:** Detailed debugging information
**trace:** Very detailed tracing

### Log Formats

#### JSON Format (Recommended)

```bash
--log-format json
```

Output:
```json
{
  "timestamp": "2024-01-22T10:30:00.123Z",
  "level": "INFO",
  "target": "fluree_db_server",
  "message": "Transaction committed",
  "fields": {
    "ledger": "mydb:main",
    "t": 42,
    "duration_ms": 45,
    "flakes_added": 3
  }
}
```

Benefits:
- Machine-parseable
- Easy to index (Elasticsearch, etc.)
- Structured fields
- JSON query tools work

#### Text Format

```bash
--log-format text
```

Output:
```text
2024-01-22T10:30:00.123Z INFO  fluree_db_server] Transaction committed ledger=mydb:main t=42 duration_ms=45
```

Benefits:
- Human-readable
- Compact
- Easy to grep

### Log Output

#### Standard Output (Default)

```bash
./fluree-db-server
```

Logs to stdout/stderr.

#### Log File

```bash
--log-file /var/log/fluree/server.log
```

```toml
[logging]
file = "/var/log/fluree/server.log"
```

#### Log Rotation

Use logrotate:

```bash
# /etc/logrotate.d/fluree
/var/log/fluree/*.log {
    daily
    rotate 14
    compress
    delaycompress
    notifempty
    create 0644 fluree fluree
    sharedscripts
    postrotate
        systemctl reload fluree
    endscript
}
```

### Structured Logging

Add context to logs:

```rust
// Rust code (for reference)
info!(
    ledger = %ledger,
    t = transaction_time,
    duration_ms = duration.as_millis(),
    "Transaction committed"
);
```

Output:
```json
{
  "message": "Transaction committed",
  "ledger": "mydb:main",
  "t": 42,
  "duration_ms": 45
}
```

## Metrics

### Prometheus Metrics

Fluree exposes Prometheus-compatible metrics:

```bash
curl http://localhost:8090/metrics
```

Output:
```text
# HELP fluree_transactions_total Total number of transactions
# TYPE fluree_transactions_total counter
fluree_transactions_total{ledger="mydb:main"} 567

# HELP fluree_queries_total Total number of queries
# TYPE fluree_queries_total counter
fluree_queries_total 12345

# HELP fluree_query_duration_seconds Query execution duration
# TYPE fluree_query_duration_seconds histogram
fluree_query_duration_seconds_bucket{le="0.01"} 8234
fluree_query_duration_seconds_bucket{le="0.05"} 11890
fluree_query_duration_seconds_bucket{le="0.1"} 12100
fluree_query_duration_seconds_sum 556.789
fluree_query_duration_seconds_count 12345

# HELP fluree_indexing_lag_transactions Indexing lag in transactions
# TYPE fluree_indexing_lag_transactions gauge
fluree_indexing_lag_transactions{ledger="mydb:main"} 5
```

### Available Metrics

**Transaction Metrics:**
- `fluree_transactions_total` - Total transactions
- `fluree_transaction_duration_seconds` - Transaction latency
- `fluree_flakes_added_total` - Total flakes added
- `fluree_flakes_retracted_total` - Total flakes retracted

**Query Metrics:**
- `fluree_queries_total` - Total queries
- `fluree_query_duration_seconds` - Query latency
- `fluree_query_errors_total` - Query errors

**Indexing Metrics:**
- `fluree_indexing_lag_transactions` - Novelty count
- `fluree_index_duration_seconds` - Indexing time
- `fluree_index_size_bytes` - Index size

**System Metrics:**
- `fluree_uptime_seconds` - Server uptime
- `fluree_memory_used_bytes` - Memory usage
- `fluree_storage_used_bytes` - Storage usage

### Prometheus Integration

Configure Prometheus to scrape Fluree:

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'fluree'
    static_configs:
      - targets: ['localhost:8090']
    metrics_path: '/metrics'
    scrape_interval: 15s
```

## Distributed Tracing (OpenTelemetry)

Fluree supports OpenTelemetry (OTEL) distributed tracing, providing deep visibility into query, transaction, and indexing performance. Traces are exported to any OTLP-compatible backend (Jaeger, Grafana Tempo, AWS X-Ray, Datadog, etc.).

### Enabling OTEL

Build the server with the `otel` feature flag:

```bash
cargo build -p fluree-db-server --features otel --release
```

Then set environment variables to configure the OTLP exporter:

```bash
OTEL_SERVICE_NAME=fluree-server \
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
RUST_LOG=info,fluree_db_query=debug,fluree_db_transact=debug \
./target/release/fluree-db-server --data-dir ./data
```

| Environment Variable | Default | Description |
|---------------------|---------|-------------|
| `OTEL_SERVICE_NAME` | `fluree-db-server` | Service name in traces |
| `OTEL_EXPORTER_OTLP_ENDPOINT` | `http://localhost:4317` | OTLP receiver endpoint |
| `OTEL_EXPORTER_OTLP_PROTOCOL` | `grpc` | Protocol: `grpc` or `http/protobuf` |

### Quick Start with Jaeger

The repository includes a self-contained test harness in the `otel/` directory:

```bash
cd otel/
make all    # starts Jaeger, builds with --features otel, starts server, runs tests
make ui     # opens Jaeger UI at http://localhost:16686
```

See [Performance Investigation with Distributed Tracing](../troubleshooting/performance-tracing.md) for detailed usage.

### Dual-Layer Subscriber Architecture

The OTEL exporter uses its own `Targets` filter **independent of `RUST_LOG`**. This is a critical design choice: without it, enabling `RUST_LOG=debug` causes third-party crate spans (hyper, tonic, h2, tower-http) to flood the OTEL batch processor, which overwhelms the exporter and causes parent spans to be dropped.

```
┌──────────────────────────────────────────────────┐
│              tracing-subscriber registry          │
│                                                   │
│  ┌─────────────────────┐  ┌────────────────────┐ │
│  │   Console fmt layer  │  │   OTEL trace layer │ │
│  │   (EnvFilter from    │  │   (Targets filter: │ │
│  │    RUST_LOG)          │  │    fluree_* only)  │ │
│  └─────────────────────┘  └────────────────────┘ │
└──────────────────────────────────────────────────┘
```

- **Console layer:** Respects `RUST_LOG` as-is (all crates)
- **OTEL layer:** Exports only `fluree_*` crate targets at DEBUG level

This means `RUST_LOG=debug` produces verbose console output, but the OTEL exporter only receives Fluree spans -- no hyper/tonic/tower noise.

### Shutdown

On server shutdown, the OTEL `SdkTracerProvider` is flushed and shut down to ensure all pending spans are exported. This is handled automatically by the server's shutdown hook.

### Span Hierarchy

Fluree instruments queries, transactions, and indexing with structured tracing spans at three tiers. All debug/trace spans are opt-in via `RUST_LOG` -- at default `info` level, only top-level operation spans appear.

#### Tier 1: INFO (always visible)

Production-level spans. Shows top-level operations and timing:

```bash
RUST_LOG=info  # default
```

Spans: `query_run`, `txn_stage`, `txn_commit`, `commit_*` sub-spans, `index_build`, `build_all_indexes`, `build_index`, `sort_blocking`, `groupby_blocking`, `join_flush_*`

#### Tier 2: DEBUG (opt-in investigation)

Phase-level decomposition. Use to identify which phase is the bottleneck:

```bash
RUST_LOG=info,fluree_db_query=debug,fluree_db_transact=debug,fluree_db_indexer=debug
```

Additional spans:
- **Query:** `query_prepare` > [`reasoning_prep`, `pattern_rewrite`, `plan`], `parse`, `format`, `policy_eval`
- **Transaction:** `txn_stage` > [`where_exec`, `delete_gen`, `insert_gen`, `cancellation`, `policy_enforce`]
- **Indexer:** `resolve_commit`, `index_gc`

#### Tier 3: TRACE (maximum detail)

Per-operator detail for deep performance analysis:

```bash
RUST_LOG=info,fluree_db_query=trace
```

Additional spans: `scan`, `join`, `property_join`, `sort`, `group_by`, `aggregate`, `group_aggregate`, `distinct`, `limit`, `offset`, `project`, `filter`, `union`, `optional`, `subquery`, `having`

#### Span Tree (Query)

```
query_prepare (debug)
├── reasoning_prep (debug)
├── pattern_rewrite (debug)
└── plan (debug)
query_run (info)
├── scan (trace)
├── join (trace)
├── project (trace)
├── sort_blocking (info)
└── ...
format (debug)
```

#### Span Tree (Transaction)

```
txn_stage (info)
├── where_exec (debug)
├── delete_gen (debug)
├── insert_gen (debug)
├── cancellation (debug)
└── policy_enforce (debug)
txn_commit (info)
├── commit_nameservice_lookup (info)
├── commit_verify_sequencing (info)
├── commit_namespace_delta (info)
├── commit_write_raw_txn (info)
├── commit_build_record (info)
├── commit_write_commit_blob (info)
├── commit_publish_nameservice (info)
├── commit_generate_metadata_flakes (info)
├── commit_clone_novelty (info)
└── commit_apply_to_novelty (info)
```

#### Span Tree (Indexing)

Indexing runs as a **separate top-level trace** (not nested under an HTTP request). Each index refresh cycle starts its own trace root:

```
index_build (info)
├── build_all_indexes (info)
│   └── build_index (info, per order: SPOT, PSOT, POST, OPST, TSOP)
├── generate_runs (info)
├── walk_commit_chain (info)
│   └── resolve_commit (debug)
└── index_gc (debug)
```

### Tracker-to-Span Bridge

When tracked queries or transactions are executed (via the `/query` or `/transact` endpoints with tracking enabled), the `tracker_time` and `tracker_fuel` fields are recorded as deferred attributes on the `query_execute` and `transact_execute` spans. These values appear as span attributes in OTEL backends (Jaeger, Tempo, etc.), enabling correlation between the Tracker's fuel accounting and the span waterfall.

### RUST_LOG Quick Reference

| Goal | Pattern |
|------|---------|
| Production default | `info` |
| Debug slow queries | `info,fluree_db_query=debug` |
| Debug slow transactions | `info,fluree_db_transact=debug` |
| Full phase decomposition | `info,fluree_db_query=debug,fluree_db_transact=debug,fluree_db_indexer=debug` |
| Per-operator detail | `info,fluree_db_query=trace` |
| Console firehose | `debug` (OTEL still filters to fluree_*) |

### Further Reading

- [Performance Investigation with Distributed Tracing](../troubleshooting/performance-tracing.md) -- How to use tracing to find bottlenecks, including AWS deployment patterns (ECS, Lambda, X-Ray, Tempo)
- [Adding Tracing Spans](../contributing/tracing-guide.md) -- How contributors should instrument new code
- [otel/ README](../../otel/README.md) -- OTEL validation harness reference

## Monitoring Integration

### Grafana Dashboards

Import Fluree dashboard:

```json
{
  "dashboard": {
    "title": "Fluree Monitoring",
    "panels": [
      {
        "title": "Query Rate",
        "targets": [
          {
            "expr": "rate(fluree_queries_total[5m])"
          }
        ]
      },
      {
        "title": "Query Latency (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, fluree_query_duration_seconds)"
          }
        ]
      },
      {
        "title": "Indexing Lag",
        "targets": [
          {
            "expr": "fluree_indexing_lag_transactions"
          }
        ]
      }
    ]
  }
}
```

### Datadog Integration

Send logs to Datadog:

```bash
./fluree-db-server \
  --log-format json | \
  datadog-agent stream --service=fluree
```

### New Relic Integration

Use New Relic agent:

```bash
export NEW_RELIC_LICENSE_KEY=your-key
export NEW_RELIC_APP_NAME=fluree-prod

./fluree-db-server
```

### Elasticsearch/Kibana

Ship logs to Elasticsearch:

```bash
./fluree-db-server \
  --log-format json | \
  filebeat -e -c filebeat.yml
```

Filebeat config:
```yaml
filebeat.inputs:
  - type: stdin
    json.keys_under_root: true

output.elasticsearch:
  hosts: ["localhost:9200"]
  index: "fluree-logs-%{+yyyy.MM.dd}"
```

## Health Monitoring

### Health Check Endpoint

```bash
curl http://localhost:8090/health
```

Response (healthy):
```json
{
  "status": "healthy",
  "version": "0.1.0",
  "storage": "file",
  "uptime_ms": 3600000,
  "checks": {
    "storage": "healthy",
    "indexing": "healthy",
    "nameservice": "healthy"
  }
}
```

Response (unhealthy):
```json
{
  "status": "unhealthy",
  "checks": {
    "storage": "healthy",
    "indexing": "unhealthy",
    "nameservice": "healthy"
  },
  "errors": [
    {
      "component": "indexing",
      "message": "Indexing lag exceeds threshold"
    }
  ]
}
```

### Liveness Probe

For Kubernetes:

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8090
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
```

### Readiness Probe

```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8090
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
```

## Alerting

### Alert Rules

Prometheus alert rules:

```yaml
groups:
  - name: fluree
    rules:
      - alert: HighQueryLatency
        expr: histogram_quantile(0.95, fluree_query_duration_seconds) > 1
        for: 5m
        annotations:
          summary: "High query latency"
          description: "95th percentile query latency is {{ $value }}s"
      
      - alert: HighIndexingLag
        expr: fluree_indexing_lag_transactions > 100
        for: 10m
        annotations:
          summary: "High indexing lag"
          description: "Indexing lag is {{ $value }} transactions"
      
      - alert: HighErrorRate
        expr: rate(fluree_query_errors_total[5m]) > 10
        for: 5m
        annotations:
          summary: "High query error rate"
          description: "Error rate is {{ $value }}/s"
```

### Alert Destinations

Configure alert routing:

```yaml
route:
  receiver: 'team-ops'
  group_by: ['alertname', 'ledger']
  routes:
    - match:
        severity: critical
      receiver: 'pagerduty'
    - match:
        severity: warning
      receiver: 'slack'

receivers:
  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'your-key'
  
  - name: 'slack'
    slack_configs:
      - api_url: 'https://hooks.slack.com/...'
        channel: '#alerts'
```

## Performance Monitoring

### Key Metrics to Track

1. **Query Performance:**
   - p50, p95, p99 latency
   - Queries per second
   - Error rate

2. **Transaction Performance:**
   - Commit time
   - Transactions per second
   - Error rate

3. **Indexing:**
   - Novelty count
   - Index time
   - Indexing lag

4. **Resource Usage:**
   - CPU utilization
   - Memory usage
   - Disk I/O
   - Network I/O

5. **Storage:**
   - Storage used
   - Storage growth rate
   - S3 request rate (if AWS)

### Dashboards

Create operational dashboards:

**Overview Dashboard:**
- Request rate
- Error rate
- Response times
- Active connections

**Performance Dashboard:**
- Query latency percentiles
- Transaction latency
- Indexing performance
- Resource utilization

**Capacity Dashboard:**
- Storage usage and growth
- Memory usage trends
- Indexing lag trends
- Projection to capacity limits

## Logging Best Practices

### 1. Use Structured Logging

JSON format with consistent fields:

```json
{
  "timestamp": "2024-01-22T10:30:00Z",
  "level": "INFO",
  "ledger": "mydb:main",
  "operation": "query",
  "duration_ms": 45
}
```

### 2. Log Request IDs

Include request IDs for tracing:

```bash
curl -X POST http://localhost:8090/query \
  -H "X-Request-ID: abc-123-def-456" \
  -d '{...}'
```

### 3. Appropriate Log Levels

- Production: `info`
- Debugging: `debug`
- Development: `debug` or `trace`

### 4. Sample High-Volume Logs

For high-traffic deployments, sample logs:

```toml
[logging]
sample_rate = 0.1  # Log 10% of requests
```

### 5. Sensitive Data

Never log sensitive data:
- API keys
- Passwords
- Personal information
- Financial data

## Related Documentation

- [Configuration](configuration.md) - Configuration options
- [Admin and Health](admin-and-health.md) - Health monitoring
- [Troubleshooting](../troubleshooting/README.md) - Debugging guides
